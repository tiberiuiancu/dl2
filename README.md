# Learning Diffusion Policies From Imagined Data
By Matthias Hagenauer, Piotr Sobecki, Tiberiu Iancu, Pablo Lozano Jimenez.

This project investigates whether synthetic demonstrations generated by DreMa—a compositional world model that creates diverse training scenes from limited data—can improve the training of diffusion policies for robotic manipulation. Diffusion policies, such as the 3D Diffusion Actor, learn multimodal action distributions via a denoising process and have shown strong performance in 3D scene-conditioned control tasks. These models typically rely on large real-world datasets, which are costly to collect. We attempt to bridge this gap by augmenting training data with DreMa-generated environments and evaluating whether this synthetic data can match or exceed the effectiveness of real demonstrations.


# Installation
The training and evaluation should be run in Apptainer (formerly Singularity) for stability across HPC systems. First, make sure to have `apptainer` installed.

```bash
> apptainer build --nv singularity.sif singularity.def
```
This results in a container file under `singularity.sif` that comes with all requirements (for training and online evaluation) preinstalled.

# Running

To train the model, see `scripts/train_keypose_drema.sh`. For evaluation, see `online_evaluation_rlbench/eval_drema.sh`. Note that these bash scripts should be run in apptainer:

```bash
> apptainer run --nv singularity.sif /bin/bash scripts/train_keypose_drema.sh
```


## Reproducibility Task 

Download the **PerAct** datasets:

* Training and validation data from the [PerAct repository](https://github.com/peract/peract)
* Test data from the [PerAct pre-generated datasets](https://github.com/peract/peract?tab=readme-ov-file#pre-generated-datasets)

Place the datasets in the following directories:

```
/scratch-shared/scur2616/Peract_packaged/train
/scratch-shared/scur2616/Peract_packaged/validation
/scratch-shared/scur2616/test3
```


## (Optional) Encode language instructions

We provide scripts for encoding language instructions with CLIP Text Encoder on CALVIN. Otherwise, you can find the encoded instructions for CALVIN and RLBench [here](https://huggingface.co/katefgroup/3d_diffuser_actor/blob/main/instructions.zip).

For encoding RLBench instructions:

```
python data_preprocessing/preprocess_rlbench_instructions.py  --tasks place_cups close_jar insert_onto_square_peg light_bulb_in meat_off_grill open_drawer place_shape_in_shape_sorter place_wine_at_rack_location push_buttons put_groceries_in_cupboard put_item_in_drawer put_money_in_safe reach_and_drag slide_block_to_color_target stack_blocks stack_cups sweep_to_dustpan_of_size turn_tap --output instructions.pkl
```

### Preprocessing

Run the following script to preprocess the PerAct data:

```
bash 3d_diffuser_actor/scripts/preprocess_peract.sh
```

### Training

Train the model on the PerAct dataset using:

```
bash 3d_diffuser_actor/scripts/train_keypose_peract.sh
```

### Evaluation

Evaluate the model on the PerAct dataset using:

```
bash 3d_diffuser_actor/online_evaluation_rlbench/eval_peract_debug.sh
```

---

## Extension task: Running the model on DreMa dataset

### Preprocessing

Run the following scripts to preprocess the DreMa dataset:

```
bash 3d_diffuser_actor/scripts/preprocess_drema.sh
bash 3d_diffuser_actor/scripts/package_drema.sh
bash 3d_diffuser_actor/scripts/package_drema_train.sh
```

### Training

Train the model on the DreMa dataset using:

```
bash 3d_diffuser_actor/scripts/train_keypose_drema.sh
```

### Evaluation

Evaluate the model on the DreMa dataset using:

```
bash 3d_diffuser_actor/online_evaluation_rlbench/eval_drema_debug.sh
```

---


# License
This code base is released under the MIT License (refer to the LICENSE file for details).

# Acknowledgement
This codebase Has been adapted from [3D Diffusion Actor](https://github.com/nickgkan/3d_diffuser_actor).
